{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardoruedas/ML/blob/main/%5B05%5D%20-%20Arboles%20de%20decision/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9eb426"
      },
      "source": [
        "# Arboles de decisión: XGBoost(Extreme Gradient Boosting) - Ejercicio 3: XGBoost.ipynb\n",
        "\n",
        "Este notebook es un **I do**: todo resuelto y explicado paso a paso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72867a89"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "- Creo un dataset alearotio con 5000 ejemplos para predecir si un cliente comprará.\n",
        "- Entrena un XGBoost.\n",
        "- Evalúa el modelo con métricas de clasificación (accuracy, matriz de confusión y reporte).\n",
        "- Muestra la importancia de cada característica (qué variables usa más el modelo para decidir)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHcmvQLHYcbB"
      },
      "source": [
        "## 1) Instalamos y cargamos librerias xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ¡IMPORTANTE! Instalar XGBoost primero:\n",
        "!pip install xgboost scikit-learn pandas numpy matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPSc0y8Rfd2g",
        "outputId": "6d89bf6b-df23-45d4-f989-1fcecff76ead"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "lSRqwY4YfQ8B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhUT3zanfzi1"
      },
      "source": [
        "## 2) Vamos a crear el dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset de clasificación (predecir si un cliente comprará)\n",
        "np.random.seed(42)\n",
        "X_clf, y_clf = make_classification(\n",
        "    n_samples=5000,  # Dataset más grande para ver diferencias\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Simulamos datos faltantes (ventaja para XGBoost!)\n",
        "# Creamos índices aleatorios para filas y columnas\n",
        "n_missing = int(0.1 * X_clf.shape[0] * X_clf.shape[1])  # 10% de valores faltantes\n",
        "missing_rows = np.random.choice(X_clf.shape[0], size=n_missing, replace=True)\n",
        "missing_cols = np.random.choice(X_clf.shape[1], size=n_missing, replace=True)\n",
        "X_clf[missing_rows, missing_cols] = np.nan\n",
        "\n",
        "print(f\"📊 Dataset creado:\")\n",
        "print(f\"   📈 Muestras: {X_clf.shape[0]:,}\")\n",
        "print(f\"   📋 Características: {X_clf.shape[1]}\")\n",
        "print(f\"   🕳️  Valores faltantes: {np.isnan(X_clf).sum():,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymuj1u6dgLUv",
        "outputId": "d84f8d04-af69-4799-a26e-e0184be31a00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Dataset creado:\n",
            "   📈 Muestras: 5,000\n",
            "   📋 Características: 20\n",
            "   🕳️  Valores faltantes: 9,490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Dividimos train y test"
      ],
      "metadata": {
        "id": "8A4WeULxgNEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.3, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "zApccnwngR75"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manejo de datos faltantes\n",
        "\n",
        "print(\"\\n Manejo de datos faltantes\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"🥊 Gradient Boosting clásico:\")\n",
        "print(\"   ❌ No puede manejar NaN directamente\")\n",
        "print(\"   🔧 Necesita preprocesamiento...\")\n",
        "\n",
        "# Para GBM clásico, rellenamos NaN\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_filled = imputer.fit_transform(X_train)\n",
        "X_test_filled = imputer.transform(X_test)\n",
        "\n",
        "print(\"   ✅ Datos rellenados con la media\")\n",
        "\n",
        "print(\"\\n⚡ XGBoost:\")\n",
        "print(\"   ✅ Maneja NaN automáticamente\")\n",
        "print(\"   🎯 Aprende la mejor estrategia para cada NaN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS04t_AggXBU",
        "outputId": "793757d3-4b5f-48bf-e026-e6c41aa5ad8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Manejo de datos faltantes\n",
            "--------------------------------------------------\n",
            "🥊 Gradient Boosting clásico:\n",
            "   ❌ No puede manejar NaN directamente\n",
            "   🔧 Necesita preprocesamiento...\n",
            "   ✅ Datos rellenados con la media\n",
            "\n",
            "⚡ XGBoost:\n",
            "   ✅ Maneja NaN automáticamente\n",
            "   🎯 Aprende la mejor estrategia para cada NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Entrenamos los modelos y comparamos"
      ],
      "metadata": {
        "id": "hZAElWA1i0rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Velocidad de entrenamiento\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Configuración similar para comparación justa\n",
        "n_estimators = 100\n",
        "max_depth = 6\n",
        "learning_rate = 0.1\n",
        "\n",
        "print(\"⏱️  Midiendo tiempos de entrenamiento...\")\n",
        "\n",
        "# Gradient Boosting clásico\n",
        "print(\"\\n🐢 Entrenando Gradient Boosting clásico...\")\n",
        "start_time = time.time()\n",
        "gbm_model = GradientBoostingClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    learning_rate=learning_rate,\n",
        "    random_state=42\n",
        ")\n",
        "gbm_model.fit(X_train_filled, y_train)\n",
        "gbm_time = time.time() - start_time\n",
        "\n",
        "# XGBoost\n",
        "print(\"🚀 Entrenando XGBoost...\")\n",
        "start_time = time.time()\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    learning_rate=learning_rate,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'  # Evita warnings\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n📊 RESULTADOS DE VELOCIDAD:\")\n",
        "print(f\"   🐢 Gradient Boosting: {gbm_time:.2f} segundos\")\n",
        "print(f\"   🚀 XGBoost:          {xgb_time:.2f} segundos\")\n",
        "print(f\"   ⚡ XGBoost es {gbm_time/xgb_time:.1f}x MÁS RÁPIDO!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A7IOlnUhiHQ",
        "outputId": "6956ca0a-f33d-4e33-b3f3-a2270ac71bdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Velocidad de entrenamiento\n",
            "--------------------------------------------------\n",
            "⏱️  Midiendo tiempos de entrenamiento...\n",
            "\n",
            "🐢 Entrenando Gradient Boosting clásico...\n",
            "🚀 Entrenando XGBoost...\n",
            "\n",
            "📊 RESULTADOS DE VELOCIDAD:\n",
            "   🐢 Gradient Boosting: 8.09 segundos\n",
            "   🚀 XGBoost:          1.04 segundos\n",
            "   ⚡ XGBoost es 7.8x MÁS RÁPIDO!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Evaluamos los resultados de los modelos tras el entrenamiento"
      ],
      "metadata": {
        "id": "lQYyaQBPjGjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precisión y rendimiento\n",
        "print(\"Precisión y rendimiento\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Predicciones\n",
        "gbm_pred = gbm_model.predict(X_test_filled)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Métricas\n",
        "gbm_accuracy = accuracy_score(y_test, gbm_pred)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "print(\"🎯 PRECISIÓN:\")\n",
        "print(f\"   🐢 Gradient Boosting: {gbm_accuracy:.4f} ({gbm_accuracy*100:.2f}%)\")\n",
        "print(f\"   🚀 XGBoost:          {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)\")\n",
        "\n",
        "if xgb_accuracy > gbm_accuracy:\n",
        "    diff = (xgb_accuracy - gbm_accuracy) * 100\n",
        "    print(f\"   🏆 XGBoost gana por +{diff:.2f} puntos porcentuales!\")\n",
        "else:\n",
        "    print(\"   🤝 ¡Empate técnico!\")\n",
        "\n",
        "# Cross-validation para ser más justos\n",
        "print(\"\\n🔄 Cross-Validation (5-fold):\")\n",
        "gbm_cv_scores = cross_val_score(gbm_model, X_train_filled, y_train, cv=5)\n",
        "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)\n",
        "\n",
        "print(f\"   🐢 GBM CV:  {gbm_cv_scores.mean():.4f} (±{gbm_cv_scores.std()*2:.4f})\")\n",
        "print(f\"   🚀 XGB CV:  {xgb_cv_scores.mean():.4f} (±{xgb_cv_scores.std()*2:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sBQLboMhkCK",
        "outputId": "0bf45879-e667-4b31-d3d1-031e5ac554df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión y rendimiento\n",
            "--------------------------------------------------\n",
            "🎯 PRECISIÓN:\n",
            "   🐢 Gradient Boosting: 0.9153 (91.53%)\n",
            "   🚀 XGBoost:          0.9147 (91.47%)\n",
            "   🤝 ¡Empate técnico!\n",
            "\n",
            "🔄 Cross-Validation (5-fold):\n",
            "   🐢 GBM CV:  0.9009 (±0.0378)\n",
            "   🚀 XGB CV:  0.8997 (±0.0309)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Vamos a interpretar los modelos"
      ],
      "metadata": {
        "id": "q_hFwuoTjT7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretabilidad\n",
        "print(\"Interpretabilidad\")\n",
        "print(\"-\" * 50)\n",
        "print(\"📊 IMPORTANCIA DE CARACTERÍSTICAS:\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gbm_importance = gbm_model.feature_importances_\n",
        "top_gbm_features = np.argsort(gbm_importance)[-5:][::-1]\n",
        "\n",
        "print(\"\\n🐢 Top 5 características - Gradient Boosting:\")\n",
        "for i, idx in enumerate(top_gbm_features):\n",
        "    print(f\"   {i+1}. Feature_{idx:2d}: {gbm_importance[idx]:.4f}\")\n",
        "\n",
        "# XGBoost (múltiples tipos de importance)\n",
        "xgb_importance_gain = xgb_model.feature_importances_\n",
        "top_xgb_features = np.argsort(xgb_importance_gain)[-5:][::-1]\n",
        "\n",
        "print(\"\\n🚀 Top 5 características - XGBoost (gain):\")\n",
        "for i, idx in enumerate(top_xgb_features):\n",
        "    print(f\"   {i+1}. Feature_{idx:2d}: {xgb_importance_gain[idx]:.4f}\")\n",
        "\n",
        "# XGBoost tiene más tipos de importance\n",
        "print(\"\\n🎯 XGBoost BONUS - Múltiples métricas de importancia:\")\n",
        "try:\n",
        "    # Importancia por frecuencia de uso\n",
        "    xgb_freq_importance = xgb_model.get_booster().get_score(importance_type='frequency')\n",
        "    print(f\"   📊 Por frecuencia disponible: {len(xgb_freq_importance)} features\")\n",
        "\n",
        "    # Importancia por ganancia\n",
        "    xgb_gain_importance = xgb_model.get_booster().get_score(importance_type='gain')\n",
        "    print(f\"   🎯 Por ganancia disponible: {len(xgb_gain_importance)} features\")\n",
        "except:\n",
        "    print(\"   ℹ️  Otras métricas disponibles con get_booster().get_score()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbHfSx1KhnoC",
        "outputId": "65ff0ad6-bfce-4976-c267-e67b12c4f10f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpretabilidad\n",
            "--------------------------------------------------\n",
            "📊 IMPORTANCIA DE CARACTERÍSTICAS:\n",
            "\n",
            "🐢 Top 5 características - Gradient Boosting:\n",
            "   1. Feature_ 9: 0.1706\n",
            "   2. Feature_ 2: 0.0865\n",
            "   3. Feature_18: 0.0805\n",
            "   4. Feature_17: 0.0751\n",
            "   5. Feature_ 8: 0.0700\n",
            "\n",
            "🚀 Top 5 características - XGBoost (gain):\n",
            "   1. Feature_ 9: 0.1488\n",
            "   2. Feature_ 2: 0.1047\n",
            "   3. Feature_18: 0.0700\n",
            "   4. Feature_17: 0.0613\n",
            "   5. Feature_15: 0.0607\n",
            "\n",
            "🎯 XGBoost BONUS - Múltiples métricas de importancia:\n",
            "   ℹ️  Otras métricas disponibles con get_booster().get_score()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Un poco sobre las caracteristicas especiales y detalles de los modelos"
      ],
      "metadata": {
        "id": "Nncq-689jque"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caracteristicas Avanzadas\n",
        "\n",
        "print(\"Caracteristicas Avanzadas\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"🔧 CARACTERÍSTICAS ESPECIALES:\")\n",
        "\n",
        "print(\"\\n🐢 Gradient Boosting clásico:\")\n",
        "print(\"   ✅ Fácil de entender\")\n",
        "print(\"   ✅ Implementación estable\")\n",
        "print(\"   ❌ Pocos parámetros avanzados\")\n",
        "print(\"   ❌ No maneja NaN\")\n",
        "print(\"   ❌ No paralelización eficiente\")\n",
        "\n",
        "print(\"\\n🚀 XGBoost:\")\n",
        "print(\"   ✅ Manejo automático de NaN\")\n",
        "print(\"   ✅ Regularización L1 y L2\")\n",
        "print(\"   ✅ Early stopping inteligente\")\n",
        "print(\"   ✅ Paralelización y GPU\")\n",
        "print(\"   ✅ Multiple importance metrics\")\n",
        "print(\"   ✅ Cross-validation integrada\")\n",
        "print(\"   ❌ Más parámetros para tunear\")\n",
        "print(\"   ❌ Puede ser overkill para datos pequeños\")\n",
        "\n",
        "# Ejemplo de early stopping con XGBoost\n",
        "print(\"\\n🛑 EARLY STOPPING en XGBoost:\")\n",
        "xgb_early = xgb.XGBClassifier(\n",
        "    n_estimators=1000,  # Muchos estimadores\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    early_stopping_rounds=10,  # Para si no mejora en 10 rounds\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Ajustar con validación\n",
        "xgb_early.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False  # Sin output detallado\n",
        ")\n",
        "\n",
        "print(f\"   📊 Estimadores usados: {xgb_early.best_iteration}\")\n",
        "print(f\"   🎯 De {1000} máximo, paró en {xgb_early.best_iteration}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXzw0iK6hq_B",
        "outputId": "3260ea0e-2827-4cfb-dc84-85e5ab5990e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caracteristicas Avanzadas\n",
            "--------------------------------------------------\n",
            "🔧 CARACTERÍSTICAS ESPECIALES:\n",
            "\n",
            "🐢 Gradient Boosting clásico:\n",
            "   ✅ Fácil de entender\n",
            "   ✅ Implementación estable\n",
            "   ❌ Pocos parámetros avanzados\n",
            "   ❌ No maneja NaN\n",
            "   ❌ No paralelización eficiente\n",
            "\n",
            "🚀 XGBoost:\n",
            "   ✅ Manejo automático de NaN\n",
            "   ✅ Regularización L1 y L2\n",
            "   ✅ Early stopping inteligente\n",
            "   ✅ Paralelización y GPU\n",
            "   ✅ Multiple importance metrics\n",
            "   ✅ Cross-validation integrada\n",
            "   ❌ Más parámetros para tunear\n",
            "   ❌ Puede ser overkill para datos pequeños\n",
            "\n",
            "🛑 EARLY STOPPING en XGBoost:\n",
            "   📊 Estimadores usados: 256\n",
            "   🎯 De 1000 máximo, paró en 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fhhrYzXVkKKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Bonus ejemplo con regresión"
      ],
      "metadata": {
        "id": "2nkgJOzjkKdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de regresión\n",
        "\n",
        "print(\"BONUS: Ejemplo de regresion\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Datos de regresión\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Modelos de regresión\n",
        "gbm_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Entrenar\n",
        "gbm_reg.fit(X_train_reg, y_train_reg)\n",
        "xgb_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predicciones\n",
        "gbm_reg_pred = gbm_reg.predict(X_test_reg)\n",
        "xgb_reg_pred = xgb_reg.predict(X_test_reg)\n",
        "\n",
        "# Métricas\n",
        "gbm_mse = mean_squared_error(y_test_reg, gbm_reg_pred)\n",
        "xgb_mse = mean_squared_error(y_test_reg, xgb_reg_pred)\n",
        "\n",
        "print(\"📈 REGRESIÓN - Mean Squared Error:\")\n",
        "print(f\"   🐢 Gradient Boosting: {gbm_mse:.4f}\")\n",
        "print(f\"   🚀 XGBoost:          {xgb_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JsnBBF4hufV",
        "outputId": "eab06a60-2896-4f7f-942a-a2765423ea90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BONUS: Ejemplo de regresion\n",
            "--------------------------------------------------\n",
            "📈 REGRESIÓN - Mean Squared Error:\n",
            "   🐢 Gradient Boosting: 1321.5928\n",
            "   🚀 XGBoost:          1854.5063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Resultados finales"
      ],
      "metadata": {
        "id": "Om5b_SO_kY-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultados finales\n",
        "\n",
        "print(\"Resultado Final\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"🏆 PUNTUACIÓN POR ROUNDS:\")\n",
        "rounds_won = 0\n",
        "\n",
        "print(\"\\n Round 1 - Manejo de NaN:\")\n",
        "print(\"   🚀 XGBoost WINS! (manejo automático)\")\n",
        "rounds_won += 1\n",
        "\n",
        "print(\"\\n Round 2 - Velocidad:\")\n",
        "if xgb_time < gbm_time:\n",
        "    print(\"   🚀 XGBoost WINS! (más rápido)\")\n",
        "    rounds_won += 1\n",
        "else:\n",
        "    print(\"   🐢 Gradient Boosting WINS!\")\n",
        "\n",
        "print(\"\\n Round 3 - Precisión:\")\n",
        "if xgb_accuracy > gbm_accuracy:\n",
        "    print(\"   🚀 XGBoost WINS! (más preciso)\")\n",
        "    rounds_won += 1\n",
        "elif gbm_accuracy > xgb_accuracy:\n",
        "    print(\"   🐢 Gradient Boosting WINS!\")\n",
        "else:\n",
        "    print(\"   🤝 EMPATE!\")\n",
        "\n",
        "print(\"\\n Round 4 - Interpretabilidad:\")\n",
        "print(\"   🚀 XGBoost WINS! (más opciones)\")\n",
        "rounds_won += 1\n",
        "\n",
        "print(\"\\n Round 5 - Características:\")\n",
        "print(\"   🚀 XGBoost WINS! (más features)\")\n",
        "rounds_won += 1\n",
        "\n",
        "print(f\"\\n🎉 RESULTADO FINAL:\")\n",
        "print(f\"   🚀 XGBoost:          {rounds_won}/5 rounds\")\n",
        "print(f\"   🐢 Gradient Boosting: {5-rounds_won}/5 rounds\")\n",
        "\n",
        "if rounds_won >= 3:\n",
        "    print(\"\\n👑 ¡XGBoost es el CAMPEÓN!\")\n",
        "    print(\"   🎯 Mejor para: competencias, datos grandes, máxima precisión\")\n",
        "else:\n",
        "    print(\"\\n👑 ¡Gradient Boosting resiste!\")\n",
        "    print(\"   🎯 Mejor para: aprendizaje, simplicidad, datasets pequeños\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1TDShLzBqhe",
        "outputId": "052b16a6-acf8-4c91-8ddf-21895056e45e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado Final\n",
            "--------------------------------------------------\n",
            "🏆 PUNTUACIÓN POR ROUNDS:\n",
            "\n",
            " Round 1 - Manejo de NaN:\n",
            "   🚀 XGBoost WINS! (manejo automático)\n",
            "\n",
            " Round 2 - Velocidad:\n",
            "   🚀 XGBoost WINS! (más rápido)\n",
            "\n",
            " Round 3 - Precisión:\n",
            "   🐢 Gradient Boosting WINS!\n",
            "\n",
            " Round 4 - Interpretabilidad:\n",
            "   🚀 XGBoost WINS! (más opciones)\n",
            "\n",
            " Round 5 - Características:\n",
            "   🚀 XGBoost WINS! (más features)\n",
            "\n",
            "🎉 RESULTADO FINAL:\n",
            "   🚀 XGBoost:          4/5 rounds\n",
            "   🐢 Gradient Boosting: 1/5 rounds\n",
            "\n",
            "👑 ¡XGBoost es el CAMPEÓN!\n",
            "   🎯 Mejor para: competencias, datos grandes, máxima precisión\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Conclusion y Consejos, cuando usar cada uno\n",
        "\n",
        "* 🐢 Gradient Boosting → Aprender conceptos, prototipos rápidos\n",
        "* 🚀 XGBoost → Producción, competencias, máximo rendimiento\n",
        "\n"
      ],
      "metadata": {
        "id": "dJkwzY4YcwT2"
      }
    }
  ]
}