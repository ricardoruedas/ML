{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardoruedas/ML/blob/main/%5B05%5D%20-%20Arboles%20de%20decision/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9eb426"
      },
      "source": [
        "# Arboles de decisiÃ³n: XGBoost(Extreme Gradient Boosting) - Ejercicio 3: XGBoost.ipynb\n",
        "\n",
        "Este notebook es un **I do**: todo resuelto y explicado paso a paso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72867a89"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "- Creo un dataset alearotio con 5000 ejemplos para predecir si un cliente comprarÃ¡.\n",
        "- Entrena un XGBoost.\n",
        "- EvalÃºa el modelo con mÃ©tricas de clasificaciÃ³n (accuracy, matriz de confusiÃ³n y reporte).\n",
        "- Muestra la importancia de cada caracterÃ­stica (quÃ© variables usa mÃ¡s el modelo para decidir)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHcmvQLHYcbB"
      },
      "source": [
        "## 1) Instalamos y cargamos librerias xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Â¡IMPORTANTE! Instalar XGBoost primero:\n",
        "!pip install xgboost scikit-learn pandas numpy matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPSc0y8Rfd2g",
        "outputId": "6d89bf6b-df23-45d4-f989-1fcecff76ead"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "lSRqwY4YfQ8B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhUT3zanfzi1"
      },
      "source": [
        "## 2) Vamos a crear el dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset de clasificaciÃ³n (predecir si un cliente comprarÃ¡)\n",
        "np.random.seed(42)\n",
        "X_clf, y_clf = make_classification(\n",
        "    n_samples=5000,  # Dataset mÃ¡s grande para ver diferencias\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Simulamos datos faltantes (ventaja para XGBoost!)\n",
        "# Creamos Ã­ndices aleatorios para filas y columnas\n",
        "n_missing = int(0.1 * X_clf.shape[0] * X_clf.shape[1])  # 10% de valores faltantes\n",
        "missing_rows = np.random.choice(X_clf.shape[0], size=n_missing, replace=True)\n",
        "missing_cols = np.random.choice(X_clf.shape[1], size=n_missing, replace=True)\n",
        "X_clf[missing_rows, missing_cols] = np.nan\n",
        "\n",
        "print(f\"ğŸ“Š Dataset creado:\")\n",
        "print(f\"   ğŸ“ˆ Muestras: {X_clf.shape[0]:,}\")\n",
        "print(f\"   ğŸ“‹ CaracterÃ­sticas: {X_clf.shape[1]}\")\n",
        "print(f\"   ğŸ•³ï¸  Valores faltantes: {np.isnan(X_clf).sum():,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymuj1u6dgLUv",
        "outputId": "d84f8d04-af69-4799-a26e-e0184be31a00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Dataset creado:\n",
            "   ğŸ“ˆ Muestras: 5,000\n",
            "   ğŸ“‹ CaracterÃ­sticas: 20\n",
            "   ğŸ•³ï¸  Valores faltantes: 9,490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Dividimos train y test"
      ],
      "metadata": {
        "id": "8A4WeULxgNEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.3, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "zApccnwngR75"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manejo de datos faltantes\n",
        "\n",
        "print(\"\\n Manejo de datos faltantes\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ¥Š Gradient Boosting clÃ¡sico:\")\n",
        "print(\"   âŒ No puede manejar NaN directamente\")\n",
        "print(\"   ğŸ”§ Necesita preprocesamiento...\")\n",
        "\n",
        "# Para GBM clÃ¡sico, rellenamos NaN\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_filled = imputer.fit_transform(X_train)\n",
        "X_test_filled = imputer.transform(X_test)\n",
        "\n",
        "print(\"   âœ… Datos rellenados con la media\")\n",
        "\n",
        "print(\"\\nâš¡ XGBoost:\")\n",
        "print(\"   âœ… Maneja NaN automÃ¡ticamente\")\n",
        "print(\"   ğŸ¯ Aprende la mejor estrategia para cada NaN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS04t_AggXBU",
        "outputId": "793757d3-4b5f-48bf-e026-e6c41aa5ad8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Manejo de datos faltantes\n",
            "--------------------------------------------------\n",
            "ğŸ¥Š Gradient Boosting clÃ¡sico:\n",
            "   âŒ No puede manejar NaN directamente\n",
            "   ğŸ”§ Necesita preprocesamiento...\n",
            "   âœ… Datos rellenados con la media\n",
            "\n",
            "âš¡ XGBoost:\n",
            "   âœ… Maneja NaN automÃ¡ticamente\n",
            "   ğŸ¯ Aprende la mejor estrategia para cada NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Entrenamos los modelos y comparamos"
      ],
      "metadata": {
        "id": "hZAElWA1i0rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Velocidad de entrenamiento\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ConfiguraciÃ³n similar para comparaciÃ³n justa\n",
        "n_estimators = 100\n",
        "max_depth = 6\n",
        "learning_rate = 0.1\n",
        "\n",
        "print(\"â±ï¸  Midiendo tiempos de entrenamiento...\")\n",
        "\n",
        "# Gradient Boosting clÃ¡sico\n",
        "print(\"\\nğŸ¢ Entrenando Gradient Boosting clÃ¡sico...\")\n",
        "start_time = time.time()\n",
        "gbm_model = GradientBoostingClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    learning_rate=learning_rate,\n",
        "    random_state=42\n",
        ")\n",
        "gbm_model.fit(X_train_filled, y_train)\n",
        "gbm_time = time.time() - start_time\n",
        "\n",
        "# XGBoost\n",
        "print(\"ğŸš€ Entrenando XGBoost...\")\n",
        "start_time = time.time()\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    learning_rate=learning_rate,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'  # Evita warnings\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nğŸ“Š RESULTADOS DE VELOCIDAD:\")\n",
        "print(f\"   ğŸ¢ Gradient Boosting: {gbm_time:.2f} segundos\")\n",
        "print(f\"   ğŸš€ XGBoost:          {xgb_time:.2f} segundos\")\n",
        "print(f\"   âš¡ XGBoost es {gbm_time/xgb_time:.1f}x MÃS RÃPIDO!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A7IOlnUhiHQ",
        "outputId": "6956ca0a-f33d-4e33-b3f3-a2270ac71bdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Velocidad de entrenamiento\n",
            "--------------------------------------------------\n",
            "â±ï¸  Midiendo tiempos de entrenamiento...\n",
            "\n",
            "ğŸ¢ Entrenando Gradient Boosting clÃ¡sico...\n",
            "ğŸš€ Entrenando XGBoost...\n",
            "\n",
            "ğŸ“Š RESULTADOS DE VELOCIDAD:\n",
            "   ğŸ¢ Gradient Boosting: 8.09 segundos\n",
            "   ğŸš€ XGBoost:          1.04 segundos\n",
            "   âš¡ XGBoost es 7.8x MÃS RÃPIDO!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Evaluamos los resultados de los modelos tras el entrenamiento"
      ],
      "metadata": {
        "id": "lQYyaQBPjGjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PrecisiÃ³n y rendimiento\n",
        "print(\"PrecisiÃ³n y rendimiento\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Predicciones\n",
        "gbm_pred = gbm_model.predict(X_test_filled)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# MÃ©tricas\n",
        "gbm_accuracy = accuracy_score(y_test, gbm_pred)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "\n",
        "print(\"ğŸ¯ PRECISIÃ“N:\")\n",
        "print(f\"   ğŸ¢ Gradient Boosting: {gbm_accuracy:.4f} ({gbm_accuracy*100:.2f}%)\")\n",
        "print(f\"   ğŸš€ XGBoost:          {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)\")\n",
        "\n",
        "if xgb_accuracy > gbm_accuracy:\n",
        "    diff = (xgb_accuracy - gbm_accuracy) * 100\n",
        "    print(f\"   ğŸ† XGBoost gana por +{diff:.2f} puntos porcentuales!\")\n",
        "else:\n",
        "    print(\"   ğŸ¤ Â¡Empate tÃ©cnico!\")\n",
        "\n",
        "# Cross-validation para ser mÃ¡s justos\n",
        "print(\"\\nğŸ”„ Cross-Validation (5-fold):\")\n",
        "gbm_cv_scores = cross_val_score(gbm_model, X_train_filled, y_train, cv=5)\n",
        "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5)\n",
        "\n",
        "print(f\"   ğŸ¢ GBM CV:  {gbm_cv_scores.mean():.4f} (Â±{gbm_cv_scores.std()*2:.4f})\")\n",
        "print(f\"   ğŸš€ XGB CV:  {xgb_cv_scores.mean():.4f} (Â±{xgb_cv_scores.std()*2:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sBQLboMhkCK",
        "outputId": "0bf45879-e667-4b31-d3d1-031e5ac554df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrecisiÃ³n y rendimiento\n",
            "--------------------------------------------------\n",
            "ğŸ¯ PRECISIÃ“N:\n",
            "   ğŸ¢ Gradient Boosting: 0.9153 (91.53%)\n",
            "   ğŸš€ XGBoost:          0.9147 (91.47%)\n",
            "   ğŸ¤ Â¡Empate tÃ©cnico!\n",
            "\n",
            "ğŸ”„ Cross-Validation (5-fold):\n",
            "   ğŸ¢ GBM CV:  0.9009 (Â±0.0378)\n",
            "   ğŸš€ XGB CV:  0.8997 (Â±0.0309)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Vamos a interpretar los modelos"
      ],
      "metadata": {
        "id": "q_hFwuoTjT7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretabilidad\n",
        "print(\"Interpretabilidad\")\n",
        "print(\"-\" * 50)\n",
        "print(\"ğŸ“Š IMPORTANCIA DE CARACTERÃSTICAS:\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gbm_importance = gbm_model.feature_importances_\n",
        "top_gbm_features = np.argsort(gbm_importance)[-5:][::-1]\n",
        "\n",
        "print(\"\\nğŸ¢ Top 5 caracterÃ­sticas - Gradient Boosting:\")\n",
        "for i, idx in enumerate(top_gbm_features):\n",
        "    print(f\"   {i+1}. Feature_{idx:2d}: {gbm_importance[idx]:.4f}\")\n",
        "\n",
        "# XGBoost (mÃºltiples tipos de importance)\n",
        "xgb_importance_gain = xgb_model.feature_importances_\n",
        "top_xgb_features = np.argsort(xgb_importance_gain)[-5:][::-1]\n",
        "\n",
        "print(\"\\nğŸš€ Top 5 caracterÃ­sticas - XGBoost (gain):\")\n",
        "for i, idx in enumerate(top_xgb_features):\n",
        "    print(f\"   {i+1}. Feature_{idx:2d}: {xgb_importance_gain[idx]:.4f}\")\n",
        "\n",
        "# XGBoost tiene mÃ¡s tipos de importance\n",
        "print(\"\\nğŸ¯ XGBoost BONUS - MÃºltiples mÃ©tricas de importancia:\")\n",
        "try:\n",
        "    # Importancia por frecuencia de uso\n",
        "    xgb_freq_importance = xgb_model.get_booster().get_score(importance_type='frequency')\n",
        "    print(f\"   ğŸ“Š Por frecuencia disponible: {len(xgb_freq_importance)} features\")\n",
        "\n",
        "    # Importancia por ganancia\n",
        "    xgb_gain_importance = xgb_model.get_booster().get_score(importance_type='gain')\n",
        "    print(f\"   ğŸ¯ Por ganancia disponible: {len(xgb_gain_importance)} features\")\n",
        "except:\n",
        "    print(\"   â„¹ï¸  Otras mÃ©tricas disponibles con get_booster().get_score()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbHfSx1KhnoC",
        "outputId": "65ff0ad6-bfce-4976-c267-e67b12c4f10f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpretabilidad\n",
            "--------------------------------------------------\n",
            "ğŸ“Š IMPORTANCIA DE CARACTERÃSTICAS:\n",
            "\n",
            "ğŸ¢ Top 5 caracterÃ­sticas - Gradient Boosting:\n",
            "   1. Feature_ 9: 0.1706\n",
            "   2. Feature_ 2: 0.0865\n",
            "   3. Feature_18: 0.0805\n",
            "   4. Feature_17: 0.0751\n",
            "   5. Feature_ 8: 0.0700\n",
            "\n",
            "ğŸš€ Top 5 caracterÃ­sticas - XGBoost (gain):\n",
            "   1. Feature_ 9: 0.1488\n",
            "   2. Feature_ 2: 0.1047\n",
            "   3. Feature_18: 0.0700\n",
            "   4. Feature_17: 0.0613\n",
            "   5. Feature_15: 0.0607\n",
            "\n",
            "ğŸ¯ XGBoost BONUS - MÃºltiples mÃ©tricas de importancia:\n",
            "   â„¹ï¸  Otras mÃ©tricas disponibles con get_booster().get_score()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Un poco sobre las caracteristicas especiales y detalles de los modelos"
      ],
      "metadata": {
        "id": "Nncq-689jque"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caracteristicas Avanzadas\n",
        "\n",
        "print(\"Caracteristicas Avanzadas\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ”§ CARACTERÃSTICAS ESPECIALES:\")\n",
        "\n",
        "print(\"\\nğŸ¢ Gradient Boosting clÃ¡sico:\")\n",
        "print(\"   âœ… FÃ¡cil de entender\")\n",
        "print(\"   âœ… ImplementaciÃ³n estable\")\n",
        "print(\"   âŒ Pocos parÃ¡metros avanzados\")\n",
        "print(\"   âŒ No maneja NaN\")\n",
        "print(\"   âŒ No paralelizaciÃ³n eficiente\")\n",
        "\n",
        "print(\"\\nğŸš€ XGBoost:\")\n",
        "print(\"   âœ… Manejo automÃ¡tico de NaN\")\n",
        "print(\"   âœ… RegularizaciÃ³n L1 y L2\")\n",
        "print(\"   âœ… Early stopping inteligente\")\n",
        "print(\"   âœ… ParalelizaciÃ³n y GPU\")\n",
        "print(\"   âœ… Multiple importance metrics\")\n",
        "print(\"   âœ… Cross-validation integrada\")\n",
        "print(\"   âŒ MÃ¡s parÃ¡metros para tunear\")\n",
        "print(\"   âŒ Puede ser overkill para datos pequeÃ±os\")\n",
        "\n",
        "# Ejemplo de early stopping con XGBoost\n",
        "print(\"\\nğŸ›‘ EARLY STOPPING en XGBoost:\")\n",
        "xgb_early = xgb.XGBClassifier(\n",
        "    n_estimators=1000,  # Muchos estimadores\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    early_stopping_rounds=10,  # Para si no mejora en 10 rounds\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Ajustar con validaciÃ³n\n",
        "xgb_early.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False  # Sin output detallado\n",
        ")\n",
        "\n",
        "print(f\"   ğŸ“Š Estimadores usados: {xgb_early.best_iteration}\")\n",
        "print(f\"   ğŸ¯ De {1000} mÃ¡ximo, parÃ³ en {xgb_early.best_iteration}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXzw0iK6hq_B",
        "outputId": "3260ea0e-2827-4cfb-dc84-85e5ab5990e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caracteristicas Avanzadas\n",
            "--------------------------------------------------\n",
            "ğŸ”§ CARACTERÃSTICAS ESPECIALES:\n",
            "\n",
            "ğŸ¢ Gradient Boosting clÃ¡sico:\n",
            "   âœ… FÃ¡cil de entender\n",
            "   âœ… ImplementaciÃ³n estable\n",
            "   âŒ Pocos parÃ¡metros avanzados\n",
            "   âŒ No maneja NaN\n",
            "   âŒ No paralelizaciÃ³n eficiente\n",
            "\n",
            "ğŸš€ XGBoost:\n",
            "   âœ… Manejo automÃ¡tico de NaN\n",
            "   âœ… RegularizaciÃ³n L1 y L2\n",
            "   âœ… Early stopping inteligente\n",
            "   âœ… ParalelizaciÃ³n y GPU\n",
            "   âœ… Multiple importance metrics\n",
            "   âœ… Cross-validation integrada\n",
            "   âŒ MÃ¡s parÃ¡metros para tunear\n",
            "   âŒ Puede ser overkill para datos pequeÃ±os\n",
            "\n",
            "ğŸ›‘ EARLY STOPPING en XGBoost:\n",
            "   ğŸ“Š Estimadores usados: 256\n",
            "   ğŸ¯ De 1000 mÃ¡ximo, parÃ³ en 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fhhrYzXVkKKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Bonus ejemplo con regresiÃ³n"
      ],
      "metadata": {
        "id": "2nkgJOzjkKdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de regresiÃ³n\n",
        "\n",
        "print(\"BONUS: Ejemplo de regresion\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Datos de regresiÃ³n\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Modelos de regresiÃ³n\n",
        "gbm_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Entrenar\n",
        "gbm_reg.fit(X_train_reg, y_train_reg)\n",
        "xgb_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predicciones\n",
        "gbm_reg_pred = gbm_reg.predict(X_test_reg)\n",
        "xgb_reg_pred = xgb_reg.predict(X_test_reg)\n",
        "\n",
        "# MÃ©tricas\n",
        "gbm_mse = mean_squared_error(y_test_reg, gbm_reg_pred)\n",
        "xgb_mse = mean_squared_error(y_test_reg, xgb_reg_pred)\n",
        "\n",
        "print(\"ğŸ“ˆ REGRESIÃ“N - Mean Squared Error:\")\n",
        "print(f\"   ğŸ¢ Gradient Boosting: {gbm_mse:.4f}\")\n",
        "print(f\"   ğŸš€ XGBoost:          {xgb_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JsnBBF4hufV",
        "outputId": "eab06a60-2896-4f7f-942a-a2765423ea90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BONUS: Ejemplo de regresion\n",
            "--------------------------------------------------\n",
            "ğŸ“ˆ REGRESIÃ“N - Mean Squared Error:\n",
            "   ğŸ¢ Gradient Boosting: 1321.5928\n",
            "   ğŸš€ XGBoost:          1854.5063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Resultados finales"
      ],
      "metadata": {
        "id": "Om5b_SO_kY-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultados finales\n",
        "\n",
        "print(\"Resultado Final\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ† PUNTUACIÃ“N POR ROUNDS:\")\n",
        "rounds_won = 0\n",
        "\n",
        "print(\"\\n Round 1 - Manejo de NaN:\")\n",
        "print(\"   ğŸš€ XGBoost WINS! (manejo automÃ¡tico)\")\n",
        "rounds_won += 1\n",
        "\n",
        "print(\"\\n Round 2 - Velocidad:\")\n",
        "if xgb_time < gbm_time:\n",
        "    print(\"   ğŸš€ XGBoost WINS! (mÃ¡s rÃ¡pido)\")\n",
        "    rounds_won += 1\n",
        "else:\n",
        "    print(\"   ğŸ¢ Gradient Boosting WINS!\")\n",
        "\n",
        "print(\"\\n Round 3 - PrecisiÃ³n:\")\n",
        "if xgb_accuracy > gbm_accuracy:\n",
        "    print(\"   ğŸš€ XGBoost WINS! (mÃ¡s preciso)\")\n",
        "    rounds_won += 1\n",
        "elif gbm_accuracy > xgb_accuracy:\n",
        "    print(\"   ğŸ¢ Gradient Boosting WINS!\")\n",
        "else:\n",
        "    print(\"   ğŸ¤ EMPATE!\")\n",
        "\n",
        "print(\"\\n Round 4 - Interpretabilidad:\")\n",
        "print(\"   ğŸš€ XGBoost WINS! (mÃ¡s opciones)\")\n",
        "rounds_won += 1\n",
        "\n",
        "print(\"\\n Round 5 - CaracterÃ­sticas:\")\n",
        "print(\"   ğŸš€ XGBoost WINS! (mÃ¡s features)\")\n",
        "rounds_won += 1\n",
        "\n",
        "print(f\"\\nğŸ‰ RESULTADO FINAL:\")\n",
        "print(f\"   ğŸš€ XGBoost:          {rounds_won}/5 rounds\")\n",
        "print(f\"   ğŸ¢ Gradient Boosting: {5-rounds_won}/5 rounds\")\n",
        "\n",
        "if rounds_won >= 3:\n",
        "    print(\"\\nğŸ‘‘ Â¡XGBoost es el CAMPEÃ“N!\")\n",
        "    print(\"   ğŸ¯ Mejor para: competencias, datos grandes, mÃ¡xima precisiÃ³n\")\n",
        "else:\n",
        "    print(\"\\nğŸ‘‘ Â¡Gradient Boosting resiste!\")\n",
        "    print(\"   ğŸ¯ Mejor para: aprendizaje, simplicidad, datasets pequeÃ±os\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1TDShLzBqhe",
        "outputId": "052b16a6-acf8-4c91-8ddf-21895056e45e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado Final\n",
            "--------------------------------------------------\n",
            "ğŸ† PUNTUACIÃ“N POR ROUNDS:\n",
            "\n",
            " Round 1 - Manejo de NaN:\n",
            "   ğŸš€ XGBoost WINS! (manejo automÃ¡tico)\n",
            "\n",
            " Round 2 - Velocidad:\n",
            "   ğŸš€ XGBoost WINS! (mÃ¡s rÃ¡pido)\n",
            "\n",
            " Round 3 - PrecisiÃ³n:\n",
            "   ğŸ¢ Gradient Boosting WINS!\n",
            "\n",
            " Round 4 - Interpretabilidad:\n",
            "   ğŸš€ XGBoost WINS! (mÃ¡s opciones)\n",
            "\n",
            " Round 5 - CaracterÃ­sticas:\n",
            "   ğŸš€ XGBoost WINS! (mÃ¡s features)\n",
            "\n",
            "ğŸ‰ RESULTADO FINAL:\n",
            "   ğŸš€ XGBoost:          4/5 rounds\n",
            "   ğŸ¢ Gradient Boosting: 1/5 rounds\n",
            "\n",
            "ğŸ‘‘ Â¡XGBoost es el CAMPEÃ“N!\n",
            "   ğŸ¯ Mejor para: competencias, datos grandes, mÃ¡xima precisiÃ³n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Conclusion y Consejos, cuando usar cada uno\n",
        "\n",
        "* ğŸ¢ Gradient Boosting â†’ Aprender conceptos, prototipos rÃ¡pidos\n",
        "* ğŸš€ XGBoost â†’ ProducciÃ³n, competencias, mÃ¡ximo rendimiento\n",
        "\n"
      ],
      "metadata": {
        "id": "dJkwzY4YcwT2"
      }
    }
  ]
}