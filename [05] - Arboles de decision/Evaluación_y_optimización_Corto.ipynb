{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardoruedas/ML/blob/main/%5B05%5D%20-%20Arboles%20de%20decision/Evaluaci%C3%B3n_y_optimizaci%C3%B3n_Corto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9eb426"
      },
      "source": [
        "# Arboles de decisión: Evaluacion y optimizacion de modelos - Ejercicio 4: Evaluación_y_optimización_Corto.ipynb\n",
        "\n",
        "Este notebook es un **I do**: todo resuelto y explicado paso a paso."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLOQUE 1 · Búsqueda de Hiperparámetros**\n",
        "\n",
        "¿Qué es un hiperparámetro?\n",
        "\n",
        "Son “mandos” externos que no aprende el modelo por sí mismo (p. ej. la profundidad máxima de un árbol, el C de una regresión logística o el número de vecinos en KNN).\n",
        "\n",
        "Elegirlos bien puede subir mucho el rendimiento; elegirlos mal te lleva a sobreajuste o bajo rendimiento."
      ],
      "metadata": {
        "id": "0EV5w7yMe84M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Grid Search (búsqueda en rejilla)\n",
        "\n",
        "Idea: probar todas las combinaciones posibles de una rejilla de valores.\n",
        "\n",
        "✅ Simple y exhaustivo.\n",
        "\n",
        "❌ Puede ser lento si hay muchos hiperparámetros/valores."
      ],
      "metadata": {
        "id": "KrfsKEB4fAkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fT_WOXie7SV",
        "outputId": "169f4053-e089-4d58-8a83-36e07ebeb815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor combinación: {'logreg__C': 1, 'logreg__penalty': 'l2'}\n",
            "Mejor F1 (CV): 0.9847673174099155\n"
          ]
        }
      ],
      "source": [
        "#Ejemplo (clasificación con Regresión Logística):\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"logreg\", LogisticRegression(max_iter=500, solver=\"saga\"))\n",
        "])\n",
        "\n",
        "\n",
        "# C controla la fuerza de la regularización (más pequeño = más regularización)\n",
        "param_grid = {\n",
        "\"logreg__penalty\": [\"l1\", \"l2\"],\n",
        "\"logreg__C\": [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "\n",
        "cv = GridSearchCV(pipe, param_grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
        "cv.fit(X, y)\n",
        "print(\"Mejor combinación:\", cv.best_params_)\n",
        "print(\"Mejor F1 (CV):\", cv.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Random Search (búsqueda aleatoria)\n",
        "\n",
        "Idea: en vez de probar todo, probamos muestras aleatorias del espacio de hiperparámetros.\n",
        "\n",
        "✅ Mucho más rápido y a menudo consigue resultados similares a grid.\n",
        "\n",
        "✅ Permite explorar rangos amplios.\n",
        "\n",
        "❌ El resultado depende del azar (aunque con suficientes iteraciones suele ir bien)."
      ],
      "metadata": {
        "id": "ZAkm2ODdfH9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo (Random Forest):\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "param_dist = {\n",
        "\"n_estimators\": [100, 200, 300, 500],\n",
        "\"max_depth\": [None, 5, 10, 15, 20],\n",
        "\"min_samples_split\": [2, 5, 10],\n",
        "\"min_samples_leaf\": [1, 2, 4],\n",
        "\"max_features\": [\"sqrt\", \"log2\", None]\n",
        "}\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "rf,\n",
        "param_distributions=param_dist,\n",
        "n_iter=30, # nº de combinaciones aleatorias a probar\n",
        "cv=5,\n",
        "scoring=\"f1\",\n",
        "n_jobs=-1,\n",
        "random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X, y)\n",
        "print(\"Mejor combinación:\", random_search.best_params_)\n",
        "print(\"Mejor F1 (CV):\", random_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtND7o70fKT8",
        "outputId": "498d929f-e466-4e69-d8cf-7bef905d944f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor combinación: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}\n",
            "Mejor F1 (CV): 0.9722580019068872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Optimización Bayesiana\n",
        "\n",
        "Idea: en lugar de probar a ciegas, un modelo probabilístico aprende qué zonas del espacio de hiperparámetros son más prometedoras y decide inteligentemente qué probar después.\n",
        "\n",
        "✅ Suele necesitar menos evaluaciones para encontrar buenos resultados.\n",
        "\n",
        "✅ Ideal cuando entrenar un modelo es caro.\n",
        "\n",
        "❌ Requiere librerías adicionales (p. ej. scikit-optimize u optuna)."
      ],
      "metadata": {
        "id": "dhJKDvMHfTho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqTLZcxVoDlu",
        "outputId": "40a80323-8620-4195-bf4c-0db83eb7611c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os-SZKJVDYXe",
        "outputId": "ca89e50c-9501-4ae2-9802-000b725e2105"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Optimización \"bayesiana\" súper rápida con Optuna (para dummies) ===\n",
        "# Si NO tienes Optuna instalada, ejecuta en una celda previa:  !pip install optuna\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# División rápida para ver desempeño final (holdout) y no sólo CV\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "\n",
        "    # Objetivo: ajustar SOLO \"C\" (suficiente para enseñar el concepto y que sea veloz)\n",
        "    def objective(trial):\n",
        "        C = trial.suggest_float(\"C\", 1e-3, 1e3, log=True)\n",
        "        # Pipeline correcto para evitar fugas y que escale dentro de cada fold\n",
        "        pipe = Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clf\", LogisticRegression(C=C, penalty=\"l2\", solver=\"lbfgs\", max_iter=500, n_jobs=None))\n",
        "        ])\n",
        "        # CV muy rápido (3 folds) y métrica f1 (mejor que accuracy en general)\n",
        "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        score = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
        "        return score\n",
        "\n",
        "    # Estudio express: 10 intentos, semilla fija para reproducibilidad\n",
        "    study = optuna.create_study(direction=\"maximize\", study_name=\"logreg_fast\", sampler=optuna.samplers.TPESampler(seed=42))\n",
        "    study.optimize(objective, n_trials=10, timeout=None)  # sube n_trials si quieres más calidad\n",
        "\n",
        "    print(\"Mejores hiperparámetros (rápidos):\", study.best_params)\n",
        "    print(\"Mejor F1 (CV 3-fold):\", round(study.best_value, 4))\n",
        "\n",
        "    # Entrena modelo final con el mejor C en TODO el train y evalúa en test\n",
        "    best_C = study.best_params[\"C\"]\n",
        "    final_pipe = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(C=best_C, penalty=\"l2\", solver=\"lbfgs\", max_iter=500))\n",
        "    ])\n",
        "    final_pipe.fit(X_train, y_train)\n",
        "    y_pred = final_pipe.predict(X_test)\n",
        "    print(\"Accuracy (test):\", round(accuracy_score(y_test, y_pred), 4))\n",
        "    print(\"F1 (test):\", round(f1_score(y_test, y_pred), 4))\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    # Fallback mínimo: sin Optuna no hacemos bayesiana; mostramos qué instalar.\n",
        "    print(\"No tienes 'optuna' instalado. Para usar la optimización bayesiana rápida, ejecuta antes:\")\n",
        "    print(\"    !pip install optuna\")\n",
        "    print(\"Luego vuelve a ejecutar esta celda.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0pC3zKp_FAH",
        "outputId": "af7ef8af-6452-41c5-f2b5-69050deeed26"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-08 18:55:17,854] A new study created in memory with name: logreg_fast\n",
            "[I 2025-10-08 18:55:17,926] Trial 0 finished with value: 0.9780390924804788 and parameters: {'C': 0.1767016940294795}. Best is trial 0 with value: 0.9780390924804788.\n",
            "[I 2025-10-08 18:55:17,976] Trial 1 finished with value: 0.9720653051089453 and parameters: {'C': 506.1576888752306}. Best is trial 0 with value: 0.9780390924804788.\n",
            "[I 2025-10-08 18:55:18,016] Trial 2 finished with value: 0.9795336384557486 and parameters: {'C': 24.658329458549105}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,055] Trial 3 finished with value: 0.9759093668929735 and parameters: {'C': 3.907967156822881}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,093] Trial 4 finished with value: 0.9623847110185908 and parameters: {'C': 0.008632008168602538}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,131] Trial 5 finished with value: 0.9623847110185908 and parameters: {'C': 0.008629132190071854}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,161] Trial 6 finished with value: 0.9335436204875881 and parameters: {'C': 0.002231010801867922}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,211] Trial 7 finished with value: 0.9795336384557486 and parameters: {'C': 157.41890047456639}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,249] Trial 8 finished with value: 0.9759093668929735 and parameters: {'C': 4.0428727350273315}. Best is trial 2 with value: 0.9795336384557486.\n",
            "[I 2025-10-08 18:55:18,298] Trial 9 finished with value: 0.9795336384557486 and parameters: {'C': 17.71884735480682}. Best is trial 2 with value: 0.9795336384557486.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores hiperparámetros (rápidos): {'C': 24.658329458549105}\n",
            "Mejor F1 (CV 3-fold): 0.9795\n",
            "Accuracy (test): 0.972\n",
            "F1 (test): 0.978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLOQUE 2 · Regularización (evitar sobreajuste)**\n",
        "\n",
        "La regularización añade una “penalización” para evitar que el modelo se complique demasiado. Imagina que pones una goma que impide que los pesos crezcan sin control."
      ],
      "metadata": {
        "id": "TQCaRuA0fcPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.1 L2 (Ridge)\n",
        "\n",
        "Penaliza la suma de cuadrados de los pesos → tiende a mantener todos los pesos pequeños.\n",
        "\n",
        "Suele estabilizar el modelo y reducir la varianza."
      ],
      "metadata": {
        "id": "DkT-T1wzfcvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo (Logistic Regression L2):\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "pipe_l2 = Pipeline([\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"clf\", LogisticRegression(penalty=\"l2\", C=1.0, solver=\"lbfgs\", max_iter=500))\n",
        "])\n",
        "cv_scores = cross_val_score(pipe_l2, X, y, cv=5, scoring=\"f1\")\n",
        "print(\"F1 medio (L2, C=1):\", cv_scores.mean())\n",
        "\n",
        "# Prueba distintos C (0.01, 0.1, 1, 10). Más pequeño = más regularización."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA0KCONNfeZ2",
        "outputId": "665bd388-f161-4ad0-927d-f4a90bb34e60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 medio (L2, C=1): 0.9847673174099155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 L1 (Lasso)\n",
        "\n",
        "Penaliza la suma de valores absolutos de los pesos → muchos pesos acaban en cero → hace selección de variables."
      ],
      "metadata": {
        "id": "xsnzR7Qpj7rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo (Logistic Regression L1):\n",
        "pipe_l1 = Pipeline([\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"clf\", LogisticRegression(penalty=\"l1\", C=0.5, solver=\"saga\", max_iter=1000))\n",
        "])\n",
        "cv_scores = cross_val_score(pipe_l1, X, y, cv=5, scoring=\"f1\")\n",
        "print(\"F1 medio (L1, C=0.5):\", cv_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xyn92SCakCgY",
        "outputId": "94d32340-a69b-4236-e65e-5b166759e414"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 medio (L1, C=0.5): 0.9778134340475815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.3 Early Stopping (parada temprana)\n",
        "\n",
        "Para el entrenamiento antes de que el modelo empiece a sobreajustar.\n",
        "\n",
        "útil en modelos que entrenan por épocas iterativas."
      ],
      "metadata": {
        "id": "_GEQPMkZkHie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo sencillo con SGDClassifier (scikit‑learn puro):\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "sgd = SGDClassifier(loss=\"log_loss\", early_stopping=True, n_iter_no_change=5, random_state=42)\n",
        "sgd.fit(X_train, y_train)\n",
        "print(\"Épocas usadas:\", sgd.n_iter_)\n",
        "print(\"F1 validación:\", f1_score(y_val, sgd.predict(X_val)))\n",
        "# En librerías de boosting (XGBoost/LightGBM/CatBoost) se usa un conjunto de validación y se detiene cuando la métrica ya no mejora tras N rondas."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIiob6bikPBk",
        "outputId": "aa57b6ae-330c-401c-d87c-85fc4ff0fe72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Épocas usadas: 7\n",
            "F1 validación: 0.9219858156028369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLOQUE 3 · Validación (comprobar que generaliza)**\n",
        "\n",
        "Meta: estimar cómo rendirá el modelo en datos nuevos."
      ],
      "metadata": {
        "id": "2EFUIVkSka_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1 Hold‑out vs K‑Fold Cross‑Validation\n",
        "\n",
        "Hold‑out: una sola división (train/test). Simple pero ruidoso.\n",
        "\n",
        "K‑Fold CV: dividir en K pliegues; entrenar K veces y promediar.\n",
        "\n",
        "Ejemplo K‑Fold:"
      ],
      "metadata": {
        "id": "koqWSuYakbgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo K‑Fold:\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "scores = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\n",
        "print(\"F1 medio (CV=5):\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glybKNEDkdPC",
        "outputId": "46b12da8-bcdd-4189-fa41-3a8fb4e029c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 medio (CV=5): 0.9327183029780179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2 Stratified Cross‑Validation\n",
        "\n",
        "En clasificación, mantener la proporción de clases en cada pliegue evita estimaciones sesgadas."
      ],
      "metadata": {
        "id": "_zSUB0ipkgfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo (comparar KFold vs StratifiedKFold):\n",
        "# Comparación KFold vs StratifiedKFold (distribución de clases por fold)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "# Datos de ejemplo (binario y desbalance moderado)\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "print(f\"Proporción positiva global: {y.mean():.3f}\\n\")\n",
        "\n",
        "kf  = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"KFold (no estratificado):\")\n",
        "for i, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "    fold_pos = y[test_idx].mean()\n",
        "    print(f\"  Fold {i+1}: % positiva = {fold_pos:.3f}\")\n",
        "\n",
        "print(\"\\nStratifiedKFold:\")\n",
        "for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
        "    fold_pos = y[test_idx].mean()\n",
        "    print(f\"  Fold {i+1}: % positiva = {fold_pos:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK_H_blFkiJl",
        "outputId": "43c3de5a-31d9-4b9c-9603-19c6f491b7c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proporción positiva global: 0.627\n",
            "\n",
            "KFold (no estratificado):\n",
            "  Fold 1: % positiva = 0.623\n",
            "  Fold 2: % positiva = 0.675\n",
            "  Fold 3: % positiva = 0.623\n",
            "  Fold 4: % positiva = 0.623\n",
            "  Fold 5: % positiva = 0.593\n",
            "\n",
            "StratifiedKFold:\n",
            "  Fold 1: % positiva = 0.623\n",
            "  Fold 2: % positiva = 0.623\n",
            "  Fold 3: % positiva = 0.632\n",
            "  Fold 4: % positiva = 0.632\n",
            "  Fold 5: % positiva = 0.628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.3 Ejemplo claro de overfitting\n",
        "\n",
        "Un árbol profundo memoriza; uno podado generaliza mejor."
      ],
      "metadata": {
        "id": "HvUV3Dq-kosB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "deep_tree = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
        "shallow_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "\n",
        "train_scores_deep = cross_val_score(deep_tree, X, y, cv=5, scoring=\"f1\")\n",
        "train_scores_shallow = cross_val_score(shallow_tree, X, y, cv=5, scoring=\"f1\")\n",
        "\n",
        "\n",
        "print(\"Árbol profundo F1 (CV):\", train_scores_deep.mean())\n",
        "print(\"Árbol podado F1 (CV):\", train_scores_shallow.mean())\n",
        "\n",
        "#Si evalúas sólo en entrenamiento, el árbol profundo parecerá “perfecto”. Con CV se ve si realmente generaliza."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aITTxkykpTA",
        "outputId": "6004a0f7-1fd2-4eca-9ab9-bddfe740876c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Árbol profundo F1 (CV): 0.9327183029780179\n",
            "Árbol podado F1 (CV): 0.9354371606953062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLOQUE 4 · Métricas esenciales**"
      ],
      "metadata": {
        "id": "5rENqrt0kyFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.1 Clasificación\n",
        "\n",
        "Accuracy: porcentaje de aciertos (cuidado con datos desbalanceados).\n",
        "\n",
        "Precision: de los que predije como positivos, ¿cuántos lo son realmente?\n",
        "\n",
        "Recall (sensibilidad): de todos los positivos reales, ¿cuántos detecté?\n",
        "\n",
        "F1: media armónica de precision y recall (equilibrio).\n",
        "\n",
        "ROC‑AUC: capacidad de ordenar bien positivos por encima de negativos.\n",
        "\n",
        "Matriz de confusión: tabla con TP, FP, FN, TN."
      ],
      "metadata": {
        "id": "9jrH_OoEGJ5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo completo de métricas:\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "proba = model.predict_proba(X_test)[:, 1]\n",
        "pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(\"Precision:\", precision_score(y_test, pred))\n",
        "print(\"Recall:\", recall_score(y_test, pred))\n",
        "print(\"F1:\", f1_score(y_test, pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, proba))\n",
        "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, pred))\n",
        "print(\"\\nReporte completo:\\n\", classification_report(y_test, pred))\n",
        "\n",
        "#Tip: ajusta el umbral (no siempre 0.5). Un umbral más bajo sube el recall a costa de bajar la precision."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98cY8LfIk0S9",
        "outputId": "dddafd58-ac3b-404a-8945-3e6f5f2f218d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.958041958041958\n",
            "Precision: 0.9565217391304348\n",
            "Recall: 0.9777777777777777\n",
            "F1: 0.967032967032967\n",
            "ROC-AUC: 0.9948637316561845\n",
            "Matriz de confusión:\n",
            " [[49  4]\n",
            " [ 2 88]]\n",
            "\n",
            "Reporte completo:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94        53\n",
            "           1       0.96      0.98      0.97        90\n",
            "\n",
            "    accuracy                           0.96       143\n",
            "   macro avg       0.96      0.95      0.95       143\n",
            "weighted avg       0.96      0.96      0.96       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 (Bonus) Métricas de Regresión\n",
        "\n",
        "MAE (error absoluto medio): fácil de interpretar.\n",
        "\n",
        "MSE / RMSE (cuadrático): penaliza grandes errores.\n",
        "\n",
        "R²: proporción de varianza explicada."
      ],
      "metadata": {
        "id": "pGd-0BgSlEkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo rápido (California Housing):\n",
        "\n",
        "# Métricas de regresión: MAE, RMSE y R² (compatible con sklearn antiguos)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Datos\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Modelo\n",
        "reg = RandomForestRegressor(n_estimators=300, random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "pred = reg.predict(X_test)\n",
        "\n",
        "# MAE\n",
        "mae = mean_absolute_error(y_test, pred)\n",
        "\n",
        "# RMSE: compatible con todas las versiones\n",
        "try:\n",
        "    # Intento rápido (sklearn recientes)\n",
        "    rmse = mean_squared_error(y_test, pred, squared=False)\n",
        "except TypeError:\n",
        "    # Fallback para sklearn antiguos\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "# R²\n",
        "r2 = r2_score(y_test, pred)\n",
        "\n",
        "print(\"MAE:\", round(mae, 4))\n",
        "print(\"RMSE:\", round(rmse, 4))\n",
        "print(\"R²:\", round(r2, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3zAFio0lE_k",
        "outputId": "99c06805-f673-4c80-9c1d-d7a2fe28706a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.3271\n",
            "RMSE: 0.502\n",
            "R²: 0.8096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cM8vGuRuM2XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}