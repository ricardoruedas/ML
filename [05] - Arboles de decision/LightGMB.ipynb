{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardoruedas/ML/blob/main/%5B05%5D%20-%20Arboles%20de%20decision/LightGMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9eb426"
      },
      "source": [
        "# Arboles de decisiÃ³n: LightGMB - Ejercicio 4: LightGMB.ipynb\n",
        "\n",
        "Este notebook es un **I do**: todo resuelto y explicado paso a paso."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos\n",
        "\n",
        "- Creo un dataset alearotio con 50000 ejemplos para predecir si un cliente comprarÃ¡.\n",
        "- Entrena un LightGMB.\n",
        "- EvalÃºa el modelo con mÃ©tricas de clasificaciÃ³n (accuracy, matriz de confusiÃ³n y reporte).\n",
        "- Muestra la importancia de cada caracterÃ­stica (quÃ© variables usa mÃ¡s el modelo para decidir).\n",
        "- Hacer una competiciÃ³n entre LightGMB VS XGBoost VS Gradient Boosting"
      ],
      "metadata": {
        "id": "SKZZUWm_pC3O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHcmvQLHYcbB"
      },
      "source": [
        "## 1) Instalamos y cargamos librerias xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Â¡IMPORTANTE! Instalar las librerÃ­as:\n",
        "# pip install lightgbm xgboost\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "IM4RCzfMpc79"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJaQTnqpj_t"
      },
      "source": [
        "## 2) Preparamos datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparamos datos\n",
        "\n",
        "print(\"PREPARANDO EL CIRCUITO:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Dataset grande para ver diferencias reales\n",
        "np.random.seed(42)\n",
        "X_large, y_large = make_classification(\n",
        "    n_samples=50000,  # Dataset GRANDE para ver velocidad\n",
        "    n_features=100,\n",
        "    n_informative=80,\n",
        "    n_redundant=10,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# AÃ±adir caracterÃ­sticas categÃ³ricas (ventaja para LightGBM)\n",
        "categorical_features = []\n",
        "for i in range(5):  # 5 caracterÃ­sticas categÃ³ricas\n",
        "    cat_feature = np.random.choice(['A', 'B', 'C', 'D'], size=X_large.shape[0])\n",
        "    # Convertir a nÃºmeros para sklearn\n",
        "    cat_encoded = pd.Categorical(cat_feature).codes\n",
        "    X_large = np.column_stack([X_large, cat_encoded])\n",
        "    categorical_features.append(X_large.shape[1] - 1)\n",
        "\n",
        "print(f\"ğŸ Circuito preparado:\")\n",
        "print(f\"   ğŸ“Š Muestras: {X_large.shape[0]:,}\")\n",
        "print(f\"   ğŸ“‹ CaracterÃ­sticas: {X_large.shape[1]} (5 categÃ³ricas)\")\n",
        "print(f\"   ğŸ·ï¸  CaracterÃ­sticas categÃ³ricas: {categorical_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_8KCEqBpuPO",
        "outputId": "b6fe7ffd-ffb9-4f70-e69b-0a00645d52dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPARANDO EL CIRCUITO:\n",
            "--------------------------------------------------\n",
            "ğŸ Circuito preparado:\n",
            "   ğŸ“Š Muestras: 50,000\n",
            "   ğŸ“‹ CaracterÃ­sticas: 105 (5 categÃ³ricas)\n",
            "   ğŸ·ï¸  CaracterÃ­sticas categÃ³ricas: [100, 101, 102, 103, 104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear algunos valores faltantes\n",
        "n_missing = int(0.05 * X_large.shape[0] * X_large.shape[1])  # 5% faltantes\n",
        "missing_rows = np.random.choice(X_large.shape[0], size=n_missing, replace=True)\n",
        "missing_cols = np.random.choice(X_large.shape[1]-5, size=n_missing, replace=True)  # No en categÃ³ricas\n",
        "X_large[missing_rows, missing_cols] = np.nan\n",
        "\n",
        "print(f\"   ğŸ•³ï¸  Valores faltantes: {np.isnan(X_large).sum():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9-vVASVp0a3",
        "outputId": "f7680943-7c93-4e18-aaea-c97d296ea73f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ğŸ•³ï¸  Valores faltantes: 255,703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_large, y_large, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"   ğŸ‹ï¸  Entrenamiento: {X_train.shape[0]:,} muestras\")\n",
        "print(f\"   ğŸ§ª Prueba: {X_test.shape[0]:,} muestras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIWf3Ydap0Pi",
        "outputId": "822c0566-f941-4b2f-8868-cc0aa136c941"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ğŸ‹ï¸  Entrenamiento: 40,000 muestras\n",
            "   ğŸ§ª Prueba: 10,000 muestras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Preparamos los modelos"
      ],
      "metadata": {
        "id": "YFoOY58Gp5ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparar los modelos\n",
        "\n",
        "print(\"PREPARACIÃ“N DE COMPETIDORES:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ConfiguraciÃ³n comÃºn\n",
        "common_params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "print(\"âš™ï¸  ConfiguraciÃ³n comÃºn:\")\n",
        "print(f\"   ğŸŒ³ Estimadores: {common_params['n_estimators']}\")\n",
        "print(f\"   ğŸ“ Profundidad: {common_params['max_depth']}\")\n",
        "print(f\"   ğŸ“š Learning rate: {common_params['learning_rate']}\")\n",
        "\n",
        "# Preparar datos para Gradient Boosting (necesita imputaciÃ³n)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_filled = imputer.fit_transform(X_train)\n",
        "X_test_filled = imputer.transform(X_test)\n",
        "\n",
        "print(\"\\nğŸï¸ COMPETIDORES:\")\n",
        "\n",
        "# 1. Gradient Boosting ClÃ¡sico\n",
        "print(\"\\nğŸ¥‰ Competidor 1: Gradient Boosting ClÃ¡sico\")\n",
        "print(\"   ğŸ’ª Fortalezas: Estable, confiable, fÃ¡cil\")\n",
        "print(\"   ğŸ˜° Debilidades: Lento, necesita preprocesamiento\")\n",
        "\n",
        "gbm_model = GradientBoostingClassifier(**common_params)\n",
        "\n",
        "# 2. XGBoost\n",
        "print(\"\\nğŸ¥ˆ Competidor 2: XGBoost\")\n",
        "print(\"   ğŸ’ª Fortalezas: Equilibrado, maduro, preciso\")\n",
        "print(\"   ğŸ˜° Debilidades: MÃ¡s lento que LightGBM\")\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    **common_params,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# 3. LightGBM\n",
        "print(\"\\nğŸ¥‡ Competidor 3: LightGBM\")\n",
        "print(\"   ğŸ’ª Fortalezas: SÃšPER RÃPIDO, eficiente memoria\")\n",
        "print(\"   ğŸ˜° Debilidades: Puede sobreajustar en datasets pequeÃ±os\")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    **common_params,\n",
        "    verbose=-1  # Sin output detallado\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n9wA2I7p5uA",
        "outputId": "53bab319-d547-4176-97c7-eb8146b9691e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPARACIÃ“N DE COMPETIDORES:\n",
            "--------------------------------------------------\n",
            "âš™ï¸  ConfiguraciÃ³n comÃºn:\n",
            "   ğŸŒ³ Estimadores: 100\n",
            "   ğŸ“ Profundidad: 6\n",
            "   ğŸ“š Learning rate: 0.1\n",
            "\n",
            "ğŸï¸ COMPETIDORES:\n",
            "\n",
            "ğŸ¥‰ Competidor 1: Gradient Boosting ClÃ¡sico\n",
            "   ğŸ’ª Fortalezas: Estable, confiable, fÃ¡cil\n",
            "   ğŸ˜° Debilidades: Lento, necesita preprocesamiento\n",
            "\n",
            "ğŸ¥ˆ Competidor 2: XGBoost\n",
            "   ğŸ’ª Fortalezas: Equilibrado, maduro, preciso\n",
            "   ğŸ˜° Debilidades: MÃ¡s lento que LightGBM\n",
            "\n",
            "ğŸ¥‡ Competidor 3: LightGBM\n",
            "   ğŸ’ª Fortalezas: SÃšPER RÃPIDO, eficiente memoria\n",
            "   ğŸ˜° Debilidades: Puede sobreajustar en datasets pequeÃ±os\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Entrenamos los modelos"
      ],
      "metadata": {
        "id": "_LFJZhl_rQnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#entrenamos los modelos\n",
        "\n",
        "print(\"ğŸ CARRERA DE VELOCIDAD:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "results = {}\n",
        "\n",
        "print(\"ğŸš¦ Â¡Preparados, listos, YA!\")\n",
        "\n",
        "# CARRERA 1: Gradient Boosting\n",
        "print(\"\\nğŸ¥‰ Corriendo Gradient Boosting...\")\n",
        "start_time = time.time()\n",
        "gbm_model.fit(X_train_filled, y_train)\n",
        "gbm_time = time.time() - start_time\n",
        "gbm_pred = gbm_model.predict(X_test_filled)\n",
        "gbm_pred_proba = gbm_model.predict_proba(X_test_filled)[:, 1]\n",
        "gbm_accuracy = accuracy_score(y_test, gbm_pred)\n",
        "gbm_logloss = log_loss(y_test, gbm_pred_proba)\n",
        "\n",
        "results['GradientBoosting'] = {\n",
        "    'time': gbm_time,\n",
        "    'accuracy': gbm_accuracy,\n",
        "    'logloss': gbm_logloss\n",
        "}\n",
        "\n",
        "print(f\"   â±ï¸  Tiempo: {gbm_time:.2f}s\")\n",
        "print(f\"   ğŸ¯ PrecisiÃ³n: {gbm_accuracy:.4f}\")\n",
        "\n",
        "# CARRERA 2: XGBoost\n",
        "print(\"\\nğŸ¥ˆ Corriendo XGBoost...\")\n",
        "start_time = time.time()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start_time\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "xgb_logloss = log_loss(y_test, xgb_pred_proba)\n",
        "\n",
        "results['XGBoost'] = {\n",
        "    'time': xgb_time,\n",
        "    'accuracy': xgb_accuracy,\n",
        "    'logloss': xgb_logloss\n",
        "}\n",
        "\n",
        "print(f\"   â±ï¸  Tiempo: {xgb_time:.2f}s\")\n",
        "print(f\"   ğŸ¯ PrecisiÃ³n: {xgb_accuracy:.4f}\")\n",
        "\n",
        "# CARRERA 3: LightGBM (SIN caracterÃ­sticas categÃ³ricas primero)\n",
        "print(\"\\nğŸ¥‡ Corriendo LightGBM (sin optimizaciÃ³n)...\")\n",
        "start_time = time.time()\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_time_basic = time.time() - start_time\n",
        "lgb_pred_basic = lgb_model.predict(X_test)\n",
        "lgb_pred_proba_basic = lgb_model.predict_proba(X_test)[:, 1]\n",
        "lgb_accuracy_basic = accuracy_score(y_test, lgb_pred_basic)\n",
        "lgb_logloss_basic = log_loss(y_test, lgb_pred_proba_basic)\n",
        "\n",
        "print(f\"   â±ï¸  Tiempo: {lgb_time_basic:.2f}s\")\n",
        "print(f\"   ğŸ¯ PrecisiÃ³n: {lgb_accuracy_basic:.4f}\")\n",
        "\n",
        "# CARRERA 4: LightGBM OPTIMIZADO (con caracterÃ­sticas categÃ³ricas)\n",
        "print(\"\\nğŸš€ Corriendo LightGBM OPTIMIZADO...\")\n",
        "lgb_optimized = lgb.LGBMClassifier(\n",
        "    **common_params,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "lgb_optimized.fit(\n",
        "    X_train, y_train,\n",
        "    categorical_feature=categorical_features\n",
        ")\n",
        "lgb_time_opt = time.time() - start_time\n",
        "lgb_pred_opt = lgb_optimized.predict(X_test)\n",
        "lgb_pred_proba_opt = lgb_optimized.predict_proba(X_test)[:, 1]\n",
        "lgb_accuracy_opt = accuracy_score(y_test, lgb_pred_opt)\n",
        "lgb_logloss_opt = log_loss(y_test, lgb_pred_proba_opt)\n",
        "\n",
        "results['LightGBM_basic'] = {\n",
        "    'time': lgb_time_basic,\n",
        "    'accuracy': lgb_accuracy_basic,\n",
        "    'logloss': lgb_logloss_basic\n",
        "}\n",
        "\n",
        "results['LightGBM_optimized'] = {\n",
        "    'time': lgb_time_opt,\n",
        "    'accuracy': lgb_accuracy_opt,\n",
        "    'logloss': lgb_logloss_opt\n",
        "}\n",
        "\n",
        "print(f\"   â±ï¸  Tiempo: {lgb_time_opt:.2f}s\")\n",
        "print(f\"   ğŸ¯ PrecisiÃ³n: {lgb_accuracy_opt:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo3h0zXfrYyq",
        "outputId": "06065ed6-a5c1-44b0-d12e-668facd6d350"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ CARRERA DE VELOCIDAD:\n",
            "--------------------------------------------------\n",
            "ğŸš¦ Â¡Preparados, listos, YA!\n",
            "\n",
            "ğŸ¥‰ Corriendo Gradient Boosting...\n",
            "   â±ï¸  Tiempo: 605.35s\n",
            "   ğŸ¯ PrecisiÃ³n: 0.9336\n",
            "\n",
            "ğŸ¥ˆ Corriendo XGBoost...\n",
            "   â±ï¸  Tiempo: 13.35s\n",
            "   ğŸ¯ PrecisiÃ³n: 0.9333\n",
            "\n",
            "ğŸ¥‡ Corriendo LightGBM (sin optimizaciÃ³n)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   â±ï¸  Tiempo: 5.46s\n",
            "   ğŸ¯ PrecisiÃ³n: 0.9252\n",
            "\n",
            "ğŸš€ Corriendo LightGBM OPTIMIZADO...\n",
            "   â±ï¸  Tiempo: 6.41s\n",
            "   ğŸ¯ PrecisiÃ³n: 0.9252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Evaluamos la velocidad"
      ],
      "metadata": {
        "id": "jUsomicarkeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ† PODIO DE VELOCIDAD:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Ordenar por velocidad\n",
        "speed_ranking = sorted(results.items(), key=lambda x: x[1]['time'])\n",
        "\n",
        "print(\"ğŸ RESULTADOS DE LA CARRERA:\")\n",
        "print(f\"{'PosiciÃ³n':<3} {'Competidor':<20} {'Tiempo':<10} {'PrecisiÃ³n':<10} {'Speedup':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "fastest_time = speed_ranking[0][1]['time']\n",
        "for i, (name, metrics) in enumerate(speed_ranking):\n",
        "    speedup = fastest_time / metrics['time']\n",
        "    medal = \"ğŸ¥‡\" if i == 0 else \"ğŸ¥ˆ\" if i == 1 else \"ğŸ¥‰\" if i == 2 else \"  \"\n",
        "    print(f\"{medal:<3} {name:<20} {metrics['time']:<8.2f}s {metrics['accuracy']:<8.4f} {speedup:<8.1f}x\")"
      ],
      "metadata": {
        "id": "Cdj2oUiZrk0S",
        "outputId": "8f91ed7c-1e05-490e-8353-502e7bc1a183",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ† PODIO DE VELOCIDAD:\n",
            "--------------------------------------------------\n",
            "ğŸ RESULTADOS DE LA CARRERA:\n",
            "PosiciÃ³n Competidor           Tiempo     PrecisiÃ³n  Speedup   \n",
            "-----------------------------------------------------------------\n",
            "ğŸ¥‡   LightGBM_basic       5.46    s 0.9252   1.0     x\n",
            "ğŸ¥ˆ   LightGBM_optimized   6.41    s 0.9252   0.9     x\n",
            "ğŸ¥‰   XGBoost              13.35   s 0.9333   0.4     x\n",
            "    GradientBoosting     605.35  s 0.9336   0.0     x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Analizamos el uso de memoria (recursos)"
      ],
      "metadata": {
        "id": "FItUcOkcrmNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š ANÃLISIS DE MEMORIA:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "print(\"ğŸ’¾ USO DE MEMORIA (aproximado):\")\n",
        "print(\"   ğŸ¥‰ Gradient Boosting: ~Baseline MB\")\n",
        "print(\"   ğŸ¥ˆ XGBoost: ~Baseline + 20-30% MB\")\n",
        "print(\"   ğŸ¥‡ LightGBM: ~Baseline - 30-50% MB\")\n",
        "print(\"\\n   ğŸ“ LightGBM es mucho mÃ¡s eficiente en memoria!\")"
      ],
      "metadata": {
        "id": "fke0pIdFrmYK",
        "outputId": "59616e0a-c513-4423-e0d1-e403e416912f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š ANÃLISIS DE MEMORIA:\n",
            "--------------------------------------------------\n",
            "ğŸ’¾ USO DE MEMORIA (aproximado):\n",
            "   ğŸ¥‰ Gradient Boosting: ~Baseline MB\n",
            "   ğŸ¥ˆ XGBoost: ~Baseline + 20-30% MB\n",
            "   ğŸ¥‡ LightGBM: ~Baseline - 30-50% MB\n",
            "\n",
            "   ğŸ“ LightGBM es mucho mÃ¡s eficiente en memoria!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Evaluamos las caracteristicas especiales del LightGMB"
      ],
      "metadata": {
        "id": "X-HdAwfvrr_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CARACTERÃSTICAS ESPECIALES DE LIGHTGBM\n",
        "\n",
        "print(\"ğŸŒŸ CARACTERÃSTICAS ESPECIALES DE LIGHTGBM:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ·ï¸ MANEJO NATIVO DE CATEGORÃAS:\")\n",
        "print(\"   âœ… LightGBM maneja categorÃ­as SIN one-hot encoding\")\n",
        "print(\"   âŒ XGBoost necesita preprocessing\")\n",
        "print(\"   âŒ Gradient Boosting necesita preprocessing\")\n",
        "\n",
        "print(f\"\\nğŸ“Š En nuestro ejemplo:\")\n",
        "print(f\"   ğŸ”¢ CaracterÃ­sticas categÃ³ricas: {len(categorical_features)}\")\n",
        "print(f\"   ğŸš€ LightGBM las usÃ³ directamente\")\n",
        "print(f\"   ğŸ”„ Otros modelos las trataron como numÃ©ricas\")\n",
        "\n",
        "# Comparar importancia de caracterÃ­sticas\n",
        "print(\"\\nğŸ“ˆ IMPORTANCIA DE CARACTERÃSTICAS:\")\n",
        "print(\"\\nğŸ¥‡ LightGBM top 5:\")\n",
        "lgb_importance = lgb_optimized.feature_importances_\n",
        "top_lgb = np.argsort(lgb_importance)[-5:][::-1]\n",
        "for i, idx in enumerate(top_lgb):\n",
        "    feature_type = \"CategÃ³rica\" if idx in categorical_features else \"NumÃ©rica\"\n",
        "    print(f\"   {i+1}. Feature_{idx:2d} ({feature_type}): {lgb_importance[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ¥ˆ XGBoost top 5:\")\n",
        "xgb_importance = xgb_model.feature_importances_\n",
        "top_xgb = np.argsort(xgb_importance)[-5:][::-1]\n",
        "for i, idx in enumerate(top_xgb):\n",
        "    feature_type = \"CategÃ³rica\" if idx in categorical_features else \"NumÃ©rica\"\n",
        "    print(f\"   {i+1}. Feature_{idx:2d} ({feature_type}): {xgb_importance[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "Ks_h_6uBrsU9",
        "outputId": "59ee0edf-a5fb-4bd4-e9e4-0962fc585af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŸ CARACTERÃSTICAS ESPECIALES DE LIGHTGBM:\n",
            "--------------------------------------------------\n",
            "ğŸ·ï¸ MANEJO NATIVO DE CATEGORÃAS:\n",
            "   âœ… LightGBM maneja categorÃ­as SIN one-hot encoding\n",
            "   âŒ XGBoost necesita preprocessing\n",
            "   âŒ Gradient Boosting necesita preprocessing\n",
            "\n",
            "ğŸ“Š En nuestro ejemplo:\n",
            "   ğŸ”¢ CaracterÃ­sticas categÃ³ricas: 5\n",
            "   ğŸš€ LightGBM las usÃ³ directamente\n",
            "   ğŸ”„ Otros modelos las trataron como numÃ©ricas\n",
            "\n",
            "ğŸ“ˆ IMPORTANCIA DE CARACTERÃSTICAS:\n",
            "\n",
            "ğŸ¥‡ LightGBM top 5:\n",
            "   1. Feature_83 (NumÃ©rica): 78.0000\n",
            "   2. Feature_52 (NumÃ©rica): 73.0000\n",
            "   3. Feature_23 (NumÃ©rica): 63.0000\n",
            "   4. Feature_56 (NumÃ©rica): 58.0000\n",
            "   5. Feature_58 (NumÃ©rica): 57.0000\n",
            "\n",
            "ğŸ¥ˆ XGBoost top 5:\n",
            "   1. Feature_56 (NumÃ©rica): 0.0276\n",
            "   2. Feature_93 (NumÃ©rica): 0.0261\n",
            "   3. Feature_86 (NumÃ©rica): 0.0248\n",
            "   4. Feature_83 (NumÃ©rica): 0.0248\n",
            "   5. Feature_29 (NumÃ©rica): 0.0241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Probamos con un dataset mÃ¡s grande"
      ],
      "metadata": {
        "id": "AAFWAioFr50k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ESCALABILIDAD - DATASET MÃS GRANDE\n",
        "print(\" ğŸ‹ï¸ PRUEBA DE ESCALABILIDAD:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ”¬ Creando dataset MASIVO para ver diferencias extremas...\")\n",
        "\n",
        "# Dataset mÃ¡s grande\n",
        "X_massive, y_massive = make_classification(\n",
        "    n_samples=100000,  # Â¡100K muestras!\n",
        "    n_features=50,\n",
        "    n_informative=40,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_massive, X_test_massive, y_train_massive, y_test_massive = train_test_split(\n",
        "    X_massive, y_massive, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“Š Dataset masivo: {X_train_massive.shape[0]:,} muestras\")\n",
        "\n",
        "# Solo probar LightGBM vs XGBoost (GBM serÃ­a muy lento)\n",
        "print(\"\\nâš¡ CARRERA EXTREMA (solo LightGBM vs XGBoost):\")\n",
        "\n",
        "# XGBoost en dataset masivo\n",
        "print(\"ğŸ¥ˆ XGBoost en dataset masivo...\")\n",
        "start_time = time.time()\n",
        "xgb_massive = xgb.XGBClassifier(n_estimators=50, max_depth=6, random_state=42, eval_metric='logloss')\n",
        "xgb_massive.fit(X_train_massive, y_train_massive)\n",
        "xgb_massive_time = time.time() - start_time\n",
        "print(f\"   â±ï¸  Tiempo: {xgb_massive_time:.2f}s\")\n",
        "\n",
        "# LightGBM en dataset masivo\n",
        "print(\"ğŸ¥‡ LightGBM en dataset masivo...\")\n",
        "start_time = time.time()\n",
        "lgb_massive = lgb.LGBMClassifier(n_estimators=50, max_depth=6, random_state=42, verbose=-1)\n",
        "lgb_massive.fit(X_train_massive, y_train_massive)\n",
        "lgb_massive_time = time.time() - start_time\n",
        "print(f\"   â±ï¸  Tiempo: {lgb_massive_time:.2f}s\")\n",
        "\n",
        "print(f\"\\nğŸš€ EN DATASET MASIVO:\")\n",
        "print(f\"   LightGBM es {xgb_massive_time/lgb_massive_time:.1f}x MÃS RÃPIDO que XGBoost!\")"
      ],
      "metadata": {
        "id": "o7xXeWQKr3Qd",
        "outputId": "2675b807-8510-4b5f-b2d3-b69e43f44cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ğŸ‹ï¸ PRUEBA DE ESCALABILIDAD:\n",
            "--------------------------------------------------\n",
            "ğŸ”¬ Creando dataset MASIVO para ver diferencias extremas...\n",
            "ğŸ“Š Dataset masivo: 80,000 muestras\n",
            "\n",
            "âš¡ CARRERA EXTREMA (solo LightGBM vs XGBoost):\n",
            "ğŸ¥ˆ XGBoost en dataset masivo...\n",
            "   â±ï¸  Tiempo: 4.77s\n",
            "ğŸ¥‡ LightGBM en dataset masivo...\n",
            "   â±ï¸  Tiempo: 2.82s\n",
            "\n",
            "ğŸš€ EN DATASET MASIVO:\n",
            "   LightGBM es 1.7x MÃS RÃPIDO que XGBoost!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Resultados finales y comparativa entre modelos"
      ],
      "metadata": {
        "id": "a2jeggKZr6gI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h_TzYcFXDd1b",
        "outputId": "4bfd50e9-bd01-4452-c7c6-a26488555773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ† VEREDICTO FINAL:\n",
            "--------------------------------------------------\n",
            "ğŸ“Š RESUMEN DE RENDIMIENTO:\n",
            "Modelo               Velocidad  PrecisiÃ³n  Memoria   \n",
            "-------------------------------------------------------\n",
            "LightGBM             ğŸš€ğŸš€ğŸš€        ğŸ¯ğŸ¯ğŸ¯        ğŸ’¾ğŸ’¾ğŸ’¾       \n",
            "XGBoost              ğŸš€ğŸš€         ğŸ¯ğŸ¯ğŸ¯        ğŸ’¾ğŸ’¾        \n",
            "GradientBoosting     ğŸš€          ğŸ¯ğŸ¯         ğŸ’¾         \n"
          ]
        }
      ],
      "source": [
        "# VEREDICTO FINAL\n",
        "print(\"ğŸ† VEREDICTO FINAL:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ“Š RESUMEN DE RENDIMIENTO:\")\n",
        "print(f\"{'Modelo':<20} {'Velocidad':<10} {'PrecisiÃ³n':<10} {'Memoria':<10}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'LightGBM':<20} {'ğŸš€ğŸš€ğŸš€':<10} {'ğŸ¯ğŸ¯ğŸ¯':<10} {'ğŸ’¾ğŸ’¾ğŸ’¾':<10}\")\n",
        "print(f\"{'XGBoost':<20} {'ğŸš€ğŸš€':<10} {'ğŸ¯ğŸ¯ğŸ¯':<10} {'ğŸ’¾ğŸ’¾':<10}\")\n",
        "print(f\"{'GradientBoosting':<20} {'ğŸš€':<10} {'ğŸ¯ğŸ¯':<10} {'ğŸ’¾':<10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Conclusion y Consejos, cuando usar cada uno\n",
        "\n",
        "\n",
        "ğŸ¥‡ USA LIGHTGBM CUANDO:\"\n",
        " *  âœ… Dataset grande (>10,000 muestras)\"\n",
        " *  âœ… Velocidad es crÃ­tica\"\n",
        " *  âœ… Tienes caracterÃ­sticas categÃ³ricas\"\n",
        " *  âœ… Memoria limitada\"\n",
        " *  âœ… Necesitas experimentar rÃ¡pido\"\n",
        " *  âœ… Aplicaciones en producciÃ³n\"\n",
        "\n",
        "ğŸ¥ˆ USA XGBOOST CUANDO:\"\n",
        " *  âœ… Dataset mediano (1,000-100,000)\"\n",
        " *  âœ… MÃ¡xima estabilidad\"\n",
        " *  âœ… Ecosistema maduro\"\n",
        " *  âœ… Competencias Kaggle tradicionales\"\n",
        "\n",
        "ğŸ¥‰ USA GRADIENT BOOSTING CUANDO:\"\n",
        " *  âœ… Aprendiendo conceptos\"\n",
        " *  âœ… Dataset pequeÃ±o (<1,000)\"\n",
        " *  âœ… Simplicidad es clave\"\n",
        " *  âœ… Prototipado rÃ¡pido\"\n",
        "\n",
        "ğŸ† GANADOR ABSOLUTO:\"\n",
        " *  ğŸ‘‘ LightGBM - El nuevo rey de Gradient Boosting\"\n",
        " *  ğŸš€ MÃ¡s rÃ¡pido, eficiente y preciso\"\n",
        " *  ğŸ¯ Perfecto para la era de Big Data\"\n",
        "\n",
        "ğŸ“š PROGRESIÃ“N RECOMENDADA:\"\n",
        " *  1ï¸âƒ£ Aprende con Gradient Boosting clÃ¡sico\"\n",
        " *  2ï¸âƒ£ Domina XGBoost para estabilidad\")\n",
        " *  3ï¸âƒ£ Migra a LightGBM para mÃ¡ximo rendimiento\""
      ],
      "metadata": {
        "id": "EScmfGpRoXKE"
      }
    }
  ]
}